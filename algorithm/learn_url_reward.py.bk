
# (Refer to [url_benchmark](https://github.com/rll-research/url_benchmark/tree/main/agent))

import os
import os.path as osp
import argparse
from argparse import ArgumentParser
import pdb
import imageio
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.autograd import Variable
import numpy as np

from attrdict import AttrDict
from distutils.util import strtobool

import os
import os.path as osp
from torch import nn

from algorithm.data_loader import ExpertDataset
from torch.utils.data import TensorDataset, DataLoader
from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler

from attrdict import AttrDict
from torchvision.utils import make_grid
from torch.utils.data import Subset
import pdb

from envs.brax_custom.brax_env import make_vec_env_brax

def process(tensor, normalize=False, range=None, scale_each=False):
    """Make a grid of images.
    Args:
        tensor (Tensor or list): 4D mini-batch Tensor of shape (B x C x H x W)
            or a list of images all of the same size.
        normalize (bool, optional): If True, shift the image to the range (0, 1),
            by the min and max values specified by :attr:`range`. Default: ``False``.
        range (tuple, optional): tuple (min, max) where min and max are numbers,
            then these numbers are used to normalize the image. By default, min and max
            are computed from the tensor.
        scale_each (bool, optional): If ``True``, scale each image in the batch of
            images separately rather than the (min, max) over all images. Default: ``False``.
    Example:
        See this notebook `here <https://gist.github.com/anonymous/bf16430f7750c023141c562f3e9f2a91>`_
    """
    if not (torch.is_tensor(tensor) or
            (isinstance(tensor, list) and all(torch.is_tensor(t) for t in tensor))):
        raise TypeError('tensor or list of tensors expected, got {}'.format(type(tensor)))

    # if list of tensors, convert to a 4D mini-batch Tensor
    if isinstance(tensor, list):
        tensor = torch.stack(tensor, dim=0)

    if tensor.dim() == 2:  # single image H x W
        tensor = tensor.unsqueeze(0)
    if tensor.dim() == 3:  # single image
        if tensor.size(0) == 1:  # if single-channel, convert to 3-channel
            tensor = torch.cat((tensor, tensor, tensor), 0)
        tensor = tensor.unsqueeze(0)

    if tensor.dim() == 4 and tensor.size(1) == 1:  # single-channel images
        tensor = torch.cat((tensor, tensor, tensor), 1)

    if normalize is True:
        tensor = tensor.clone()  # avoid modifying tensor in-place
        if range is not None:
            assert isinstance(range, tuple), \
                "range has to be a tuple (min, max) if specified. min and max are numbers"

        def norm_ip(img, min, max):
            img.clamp_(min=min, max=max)
            img.add_(-min).div_(max - min + 1e-5)

        def norm_range(t, range):
            if range is not None:
                norm_ip(t, range[0], range[1])
            else:
                norm_ip(t, float(t.min()), float(t.max()))

        if scale_each is True:
            for t in tensor:  # loop over mini-batch dimension
                norm_range(t, range)
        else:
            norm_range(tensor, range)

    return tensor 

class RMS(object):
    """running mean and std """
    def __init__(self, device, epsilon=1e-4, shape=(1,)):
        self.M = torch.zeros(shape).to(device)
        self.S = torch.ones(shape).to(device)
        self.n = epsilon

    def __call__(self, x):
        bs = x.size(0)
        delta = torch.mean(x, dim=0) - self.M
        new_M = self.M + delta * bs / (self.n + bs)
        new_S = (self.S * self.n + torch.var(x, dim=0) * bs +
                 torch.square(delta) * self.n * bs /
                 (self.n + bs)) / (self.n + bs)

        self.M = new_M
        self.S = new_S
        self.n += bs

        return self.M, self.S


class PBE(object):
    """particle-based entropy based on knn normalized by running mean """
    def __init__(self, rms, knn_clip, knn_k, knn_avg, knn_rms, device):
        self.rms = rms
        self.knn_rms = knn_rms
        self.knn_k = knn_k
        self.knn_avg = knn_avg
        self.knn_clip = knn_clip
        self.device = device

    def __call__(self, rep):
        source = target = rep
        b1, b2 = source.size(0), target.size(0)
        # (b1, 1, c) - (1, b2, c) -> (b1, 1, c) - (1, b2, c) -> (b1, b2, c) -> (b1, b2)
        sim_matrix = torch.norm(source[:, None, :].view(b1, 1, -1) -
                                target[None, :, :].view(1, b2, -1),
                                dim=-1,
                                p=2)
        # avoid index out of range
        self.knn_k = min(self.knn_k, sim_matrix.shape[0])
        reward, _ = sim_matrix.topk(self.knn_k,
                                    dim=1,
                                    largest=False,
                                    sorted=True)  # (b1, k)
        if not self.knn_avg:  # only keep k-th nearest neighbor
            reward = reward[:, -1]
            reward = reward.reshape(-1, 1)  # (b1, 1)
            reward /= self.rms(reward)[0] if self.knn_rms else 1.0
            reward = torch.maximum(
                reward - self.knn_clip,
                torch.zeros_like(reward).to(self.device)
            ) if self.knn_clip >= 0.0 else reward  # (b1, 1)
        else:  # average over all k nearest neighbors
            reward = reward.reshape(-1, 1)  # (b1 * k, 1)
            reward /= self.rms(reward)[0] if self.knn_rms else 1.0
            reward = torch.maximum(
                reward - self.knn_clip,
                torch.zeros_like(reward).to(
                    self.device)) if self.knn_clip >= 0.0 else reward
            reward = reward.reshape((b1, self.knn_k))  # (b1, k)
            reward = reward.mean(dim=1, keepdim=True)  # (b1, 1)
        reward = torch.log(reward + 1.0)
        return reward

class RunningMeanStd(object):
    # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Parallel_algorithm
    def __init__(self, epsilon=1e-4, shape=()):
        self.mean = np.zeros(shape, 'float64')
        self.var = np.ones(shape, 'float64')
        self.count = epsilon

    def update(self, x):
        if np.isnan(x).any():
            print('nan value occures in input x, convert to numbers')
            x = np.nan_to_num(x)
        batch_mean = np.mean(x, axis=0)
        batch_var = np.var(x, axis=0)
        batch_count = x.shape[0]
        self.update_from_moments(batch_mean, batch_var, batch_count)

    def update_from_moments(self, batch_mean, batch_var, batch_count):
        self.mean, self.var, self.count = update_mean_var_count_from_moments(
            self.mean, self.var, self.count, batch_mean, batch_var, batch_count)

def update_mean_var_count_from_moments(mean, var, count, batch_mean, batch_var, batch_count):
    if np.isnan(batch_mean).any():
        print('nan value occures in batch_mean, convert to numbers')
        batch_mean = np.nan_to_num(batch_mean)
    if np.isnan(mean).any():
        print('nan value occures in mean, convert to numbers')
        mean = np.nan_to_num(mean)
    delta = batch_mean - mean
    tot_count = count + batch_count

    new_mean = mean + delta * batch_count / tot_count
    m_a = var * count
    m_b = batch_var * batch_count
    M2 = m_a + m_b + np.square(delta) * count * batch_count / tot_count
    new_var = M2 / tot_count
    new_count = tot_count

    return new_mean, new_var, new_count


# current results only use 10% data, to rerun the experiments with 100% data using the following corrected code.
def load_sa_data(args, return_next_state=True):
    traj_root = args.demo_dir
    traj_file = f'{traj_root}/trajs_ppga_{args.env_name}.pt'
    print(f'Loading data: {traj_file}')
    dataset = ExpertDataset(file_name=traj_file, num_trajectories=args.num_demo, train=True, 
                            train_test_split=1.0, return_next_state=return_next_state)
    dataloader = DataLoader(dataset, batch_size=args.num_minibatches, shuffle=False, num_workers=1, drop_last=False)
    return dataset, dataloader


class Encoder(nn.Module):

    def __init__(self, latent_dim, action_dim,
                 hidden_dim=64):

        super(Encoder, self).__init__()
        self.input_dim = latent_dim
        self.output_dim = action_dim
        self.hidden = hidden_dim

        # Inverse Model architecture
        self.linear_1 = nn.Linear(in_features=self.input_dim*2, out_features=64)
        self.linear_2 = nn.Linear(in_features=64, out_features=self.hidden)
        # self.output = nn.Linear(in_features=self.hidden, out_features=self.output_dim)
        # # Leaky relu activation
        self.lrelu_1 = nn.LeakyReLU()
        self.lrelu_2 = nn.LeakyReLU()

        self.mu = nn.Linear(hidden_dim, action_dim)
        self.logvar = nn.Linear(hidden_dim, action_dim)

        # Initialize the weights using xavier initialization
        nn.init.xavier_uniform_(self.linear_1.weight)
        nn.init.xavier_uniform_(self.linear_2.weight)
        # nn.init.xavier_uniform_(self.output.weight)
        nn.init.xavier_uniform_(self.mu.weight)
        nn.init.xavier_uniform_(self.logvar.weight)

    def reparameterize(self, mu, logvar, device, training=True):
        # Reparameterization trick as shown in the auto encoding variational bayes paper
        if training:
            std = logvar.mul(0.5).exp_()
            eps = Variable(std.data.new(std.size()).normal_()).to(device)
            return eps.mul(std).add_(mu)
        else:
            return mu

    def forward(self, state, next_state):

        # Concatenate the state and the next state
        input = torch.cat([state, next_state], dim=-1)
        x = self.linear_1(input)
        x = self.lrelu_1(x)
        x = self.linear_2(x)
        x = self.lrelu_2(x)
        # x = self.output(x)

        mu = self.mu(x)
        logvar = self.logvar(x)
        
        z = self.reparameterize(mu, logvar, state.device)
        return z, mu, logvar

### RND model
def weight_init(m):
    """Custom weight init for Conv2D and Linear layers."""
    if isinstance(m, nn.Linear):
        nn.init.orthogonal_(m.weight.data)
        if hasattr(m.bias, 'data'):
            m.bias.data.fill_(0.0)
    elif isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):
        gain = nn.init.calculate_gain('relu')
        nn.init.orthogonal_(m.weight.data, gain)
        if hasattr(m.bias, 'data'):
            m.bias.data.fill_(0.0)

class Predictor(nn.Module):
    def __init__(self, latent_dim, rnd_rep_dim=64,
                 hidden_dim=64):

        super(Predictor, self).__init__()
        self.input_dim = latent_dim
        self.output_dim = rnd_rep_dim
        self.hidden = hidden_dim

        # Model architecture
        self.linear_1 = nn.Linear(in_features=self.input_dim, out_features=self.hidden)
        # self.linear_2 = nn.Linear(in_features=1024, out_features=self.hidden)
        self.output = nn.Linear(in_features=self.hidden, out_features=self.output_dim)

        # Leaky relu activation
        self.lrelu_1 = nn.ReLU()
        # self.lrelu_2 = nn.ReLU()

        # Initialize the weights using xavier initialization
        self.apply(weight_init)

    def forward(self, state):
        x = self.linear_1(state)
        x = self.lrelu_1(x)
        # x = self.linear_2(x)
        # x = self.lrelu_2(x)
        x = self.output(x)
        return x

class Target(nn.Module):
    def __init__(self, latent_dim, rnd_rep_dim=64,
                 hidden_dim=64):

        super(Target, self).__init__()
        self.input_dim = latent_dim
        self.output_dim = rnd_rep_dim
        self.hidden = hidden_dim

        # Model architecture
        self.linear_1 = nn.Linear(in_features=self.input_dim, out_features=self.hidden)
        # self.linear_2 = nn.Linear(in_features=1024, out_features=self.hidden)
        self.output = nn.Linear(in_features=self.hidden, out_features=self.output_dim)

        # Leaky relu activation
        self.lrelu_1 = nn.ReLU()
        # self.lrelu_2 = nn.ReLU()

        # Initialize the weights using xavier initialization
        self.apply(weight_init)

    def forward(self, state):
        x = self.linear_1(state)
        x = self.lrelu_1(x)
        # x = self.linear_2(x)
        # x = self.lrelu_2(x)
        x = self.output(x)
        return x

#### ICM model
# Inverse Dynamics model
class InverseModel(nn.Module):

    def __init__(self, latent_dim, action_dim,
                 hidden_dim):

        super(InverseModel, self).__init__()
        self.input_dim = latent_dim
        self.output_dim = action_dim
        self.hidden = hidden_dim

        # Inverse Model architecture
        self.linear_1 = nn.Linear(in_features=self.input_dim*2, out_features=self.hidden)
        # self.linear_2 = nn.Linear(in_features=1024, out_features=self.hidden)
        self.output = nn.Linear(in_features=self.hidden, out_features=self.output_dim)

        # Leaky relu activation
        self.lrelu_1 = nn.LeakyReLU()
        # self.lrelu_2 = nn.LeakyReLU()

        # Output Activation
        self.softmax = nn.Softmax()

        # Initialize the weights using xavier initialization
        nn.init.xavier_uniform_(self.linear_1.weight)
        # nn.init.xavier_uniform_(self.linear_2.weight)
        nn.init.xavier_uniform_(self.output.weight)

    def forward(self, state, next_state):

        # Concatenate the state and the next state
        input = torch.cat([state, next_state], dim=-1)
        x = self.linear_1(input)
        x = self.lrelu_1(x)
        # x = self.linear_2(x)
        # x = self.lrelu_2(x)
        x = self.output(x)
        #output = self.softmax(x)
        return x


# Forward Dynamics Model
class ForwardDynamicsModel(nn.Module):

    def __init__(self, state_dim, action_dim,
                 hidden_dim):

        super(ForwardDynamicsModel, self).__init__()

        self.input_dim = state_dim+action_dim
        self.output_dim= state_dim
        self.hidden = hidden_dim


        # Forward Model Architecture
        self.linear_1 = nn.Linear(in_features=self.input_dim, out_features=self.hidden)
        # self.linear_2 = nn.Linear(in_features=self.hidden, out_features=1024)
        self.output = nn.Linear(in_features=self.hidden, out_features=self.output_dim)

        # Leaky Relu activation
        self.lrelu_1 = nn.LeakyReLU()
        # self.lrelu_2 = nn.LeakyReLU()

        # Initialize the weights using xavier initialization
        nn.init.xavier_uniform_(self.linear_1.weight)
        # nn.init.xavier_uniform_(self.linear_2.weight)
        nn.init.xavier_uniform_(self.output.weight)

    def forward(self, state, action):
        # Concatenate the state and the action
        # Note that the state in this case is the feature representation of the state
        input = torch.cat([state, action], dim=-1)
        x = self.linear_1(input)
        x = self.lrelu_1(x)
        # x = self.linear_2(x)
        # x = self.lrelu_2(x)
        output = self.output(x)

        return output

class RandomShiftsAug(nn.Module):
    def __init__(self, pad):
        super().__init__()
        self.pad = pad

    def forward(self, x):
        x = x.float()
        n, c, h, w = x.size()
        assert h == w
        padding = tuple([self.pad] * 4)
        x = F.pad(x, padding, 'replicate')
        eps = 1.0 / (h + 2 * self.pad)
        arange = torch.linspace(-1.0 + eps,
                                1.0 - eps,
                                h + 2 * self.pad,
                                device=x.device,
                                dtype=x.dtype)[:h]
        arange = arange.unsqueeze(0).repeat(h, 1).unsqueeze(2)
        base_grid = torch.cat([arange, arange.transpose(1, 0)], dim=2)
        base_grid = base_grid.unsqueeze(0).repeat(n, 1, 1, 1)

        shift = torch.randint(0,
                              2 * self.pad + 1,
                              size=(n, 1, 1, 2),
                              device=x.device,
                              dtype=x.dtype)
        shift *= 2.0 / (h + 2 * self.pad)

        grid = base_grid + shift
        return F.grid_sample(x,
                             grid,
                             padding_mode='zeros',
                             align_corners=False)

class Disagreement(nn.Module):
    def __init__(self, obs_dim, action_dim, hidden_dim, n_models=5):
        super().__init__()
        self.ensemble = nn.ModuleList([
            nn.Sequential(nn.Linear(obs_dim + action_dim, hidden_dim),
                          nn.ReLU(), nn.Linear(hidden_dim, obs_dim))
            for _ in range(n_models)
        ])

    def forward(self, obs, action, next_obs):
        #import ipdb; ipdb.set_trace()
        assert obs.shape[0] == next_obs.shape[0]
        assert obs.shape[0] == action.shape[0]

        errors = []
        for model in self.ensemble:
            
            next_obs_hat = model(torch.cat([obs, action], dim=-1))
            model_error = torch.norm(next_obs - next_obs_hat,
                                     dim=-1,
                                     p=2,
                                     keepdim=True)
            errors.append(model_error)

        return torch.cat(errors, dim=1)

    def get_disagreement(self, obs, action, next_obs):
        assert obs.shape[0] == next_obs.shape[0]
        assert obs.shape[0] == action.shape[0]
        if len(action.shape) == 3:
            action = action.squeeze(1)

        preds = []
        for model in self.ensemble:
            next_obs_hat = model(torch.cat([obs, action], dim=-1))
            preds.append(next_obs_hat)
        preds = torch.stack(preds, dim=0)
        return torch.var(preds, dim=0).mean(dim=-1)

class Disagreement(object):
    def __init__(self,
                 obs_dim,
                 action_dim,
                 device='cuda:0',
                 lr=3e-4,
                 ):

        self.device = device

        self.disagreement = Disagreement(obs_dim=obs_dim, action_dim=action_dim,
                                         hidden_dim=64).to(device)

        self.lr = lr
        self.intrinsic_reward_rms = RMS(device=self.device)

        self.disagreement_optim = optim.Adam(
                                [
                                    {'params': self.disagreement.parameters()}
                                ],
                                lr=self.lr
                                )
        
        self.normalize_obs = nn.BatchNorm1d(obs_dim, affine=False).to(self.device)

    def fit_batch(self, state, action, next_state, train=True):
        state_feature = state
        next_state_feature = next_state

        error = self.disagreement(state_feature, action, next_state_feature)

        loss = error.mean()

        if train:
            self.disagreement_optim.zero_grad(set_to_none=True)
            loss.backward()
            self.disagreement_optim.step()
        return loss

    # Calculation of the curiosity reward
    def calculate_intrinsic_reward(self, state, action, next_state, reduction='none'):
        with torch.no_grad():
            state_feature = state
            next_state_feature = next_state

            reward = self.disagreement.get_disagreement(state_feature, action,
                                                        next_state_feature)

        return reward #.squeeze(1)



### Random Network Distillation
# Exploration by Random Network Distillation https://arxiv.org/abs/1810.12894
# https://github.com/rll-research/url_benchmark/blob/main/agent/rnd.py
class RND(object):
    def __init__(self,
                 obs_dim,
                 rnd_rep_dim=512,
                 clip_val=5.,
                 device='cuda:0',
                 lr=3e-4,
                 ):

        self.device = device
        self.aug = nn.Identity()
        self.clip_val = clip_val
        latent_dim = obs_dim
        
        self.predictor = Predictor(latent_dim=latent_dim, rnd_rep_dim=rnd_rep_dim, \
                                     hidden_dim=64).to(device)
        self.target = Target(latent_dim=latent_dim, rnd_rep_dim=rnd_rep_dim, \
                                    hidden_dim=64).to(device)
        for param in self.target.parameters():
            param.requires_grad = False

        self.lr = lr
        self.intrinsic_reward_rms = RMS(device=self.device)

        self.rnd_optim = optim.Adam(
                                [
                                    {'params': self.predictor.parameters()},
                                    {'params': self.target.parameters()}
                                ],
                                lr=self.lr
                                )
        
        self.normalize_obs = nn.BatchNorm1d(latent_dim, affine=False).to(self.device)

    def fit_batch(self, state, action, next_state, train=True):
        state_feature = state

        state_feature = self.aug(state_feature)
        state_feature = self.normalize_obs(state_feature)
        state_feature = torch.clamp(state_feature, -self.clip_val, self.clip_val)

        prediction, target = self.predictor(state_feature), self.target(state_feature)
        prediction_error = torch.square(target.detach() - prediction).mean(
                                                                dim=-1, keepdim=True)
        
        loss = prediction_error.mean()

        if train:
            self.rnd_optim.zero_grad(set_to_none=True)
            loss.backward()
            self.rnd_optim.step()
        return loss

    # Calculation of the curiosity reward
    def calculate_intrinsic_reward(self, state, action, next_state, reduction='none'):
        with torch.no_grad():
            state_feature = state

            state_feature = self.aug(state_feature)
            state_feature = self.normalize_obs(state_feature)
            state_feature = torch.clamp(state_feature, -self.clip_val, self.clip_val)

            prediction, target = self.predictor(state_feature), self.target(state_feature)
            prediction_error = torch.square(target.detach() - prediction).mean(
                                                                    dim=-1, keepdim=True)
            
            _, intr_reward_var = self.intrinsic_reward_rms(prediction_error)
            reward =  prediction_error / (
                torch.sqrt(intr_reward_var) + 1e-8)

        return reward.squeeze(1)

### TODO: check and fix VCSE, GAIL, and VAIL for mujoco 
### Accelerating Reinforcement Learning with Value-Conditional State Entropy Exploration
# https://sites.google.com/view/rl-vcse
class VCSE(object):
    def __init__(self, knn_k):
        self.knn_k = knn_k

    def __call__(self, state,value):
        #value => [b1 , 1]
        #state => [b1 , c]
        #z => [b1, c+1]
        # [b1] => [b1,b1]
        ds = state.size(1)
        source = target = state
        b1, b2 = source.size(0), target.size(0)
        # (b1, 1, c+1) - (1, b2, c+1) -> (b1, 1, c+1) - (1, b2, c+1) -> (b1, b2, c+1) -> (b1, b2)
        sim_matrix_s = torch.norm(source[:, None, :].view(b1, 1, -1) -
                                target[None, :, :].view(1, b2, -1),
                                dim=-1,
                                p=2)

        source = target = value
        # (b1, 1, 1) - (1, b2, 1) -> (b1, 1, 1) - (1, b2, 1) -> (b1, b2, 1) -> (b1, b2)
        sim_matrix_v = torch.norm(source[:, None, :].view(b1, 1, -1) -
                                target[None, :, :].view(1, b2, -1),
                                dim=-1,
                                p=2)
        
        sim_matrix = torch.max(torch.cat((sim_matrix_s.unsqueeze(-1),sim_matrix_v.unsqueeze(-1)),dim=-1),dim=-1)[0]
        eps, index = sim_matrix.topk(self.knn_k,
                                    dim=1,
                                    largest=False,
                                    sorted=True)  # (b1, k)
        
        state_norm, index = sim_matrix_s.topk(self.knn_k,
                                    dim=1,
                                    largest=False,
                                    sorted=True)  # (b1, k)
        
        value_norm, index = sim_matrix_v.topk(self.knn_k,
                                    dim=1,
                                    largest=False,
                                    sorted=True)  # (b1, k)
        
        eps = eps[:, -1] #k-th nearest distance
        eps = eps.reshape(-1, 1) # (b1, 1)
        
        state_norm = state_norm[:, -1] #k-th nearest distance
        state_norm = state_norm.reshape(-1, 1) # (b1, 1)

        value_norm = value_norm[:, -1] #k-th nearest distance
        value_norm = value_norm.reshape(-1, 1) # (b1, 1)
        
        sim_matrix_v = sim_matrix_v < eps
        n_v = torch.sum(sim_matrix_v,dim=1,keepdim = True) # (b1,1)
        
        sim_matrix_s = sim_matrix_s < eps
        n_s = torch.sum(sim_matrix_s,dim=1,keepdim = True) # (b1,1)        

        reward = torch.digamma(n_v+1) / ds + torch.log(eps * 2 + 0.00001)
        return reward, n_v,n_s, eps, state_norm, value_norm  


# GAIL and VAIL

# GAIL discriminator
class GAILdiscriminator(nn.Module):

    def __init__(self, latent_dim, 
                 hidden_dim, action_dim):

        super(GAILdiscriminator, self).__init__()
        self.input_dim = latent_dim
        self.output_dim = 1
        self.hidden = hidden_dim
        self.action_dim = action_dim 

        # discriminator architecture
        self.linear_1 = nn.Linear(in_features=self.input_dim+self.action_dim, out_features=1024)
        self.linear_2 = nn.Linear(in_features=1024, out_features=self.hidden)
        self.output = nn.Linear(in_features=self.hidden, out_features=self.output_dim)

        # Leaky relu activation
        self.lrelu_1 = nn.LeakyReLU()
        self.lrelu_2 = nn.LeakyReLU()

        # Output Activation
        # self.softmax = nn.Softmax()

        # Initialize the weights using xavier initialization
        nn.init.xavier_uniform_(self.linear_1.weight)
        nn.init.xavier_uniform_(self.linear_2.weight)
        nn.init.xavier_uniform_(self.output.weight)

    def forward(self, x):
        x = self.linear_1(x)
        x = self.lrelu_1(x)
        x = self.linear_2(x)
        x = self.lrelu_2(x)
        x = self.output(x)
        return x

# VAIL discriminator
class VAILdiscriminator(nn.Module):

    def __init__(self, latent_dim, 
                 hidden_dim, action_dim):

        super(VAILdiscriminator, self).__init__()
        self.input_dim = latent_dim
        self.output_dim = 1
        self.hidden = hidden_dim
        self.action_dim = action_dim 

        # Inverse Model architecture
        self.linear_1 = nn.Linear(in_features=self.input_dim+self.action_dim, out_features=1024)
        self.linear_2 = nn.Linear(in_features=1024, out_features=self.hidden)
        self.output = nn.Linear(in_features=int(self.hidden/2), out_features=self.output_dim)

        # Leaky relu activation
        self.lrelu_1 = nn.LeakyReLU()
        self.lrelu_2 = nn.LeakyReLU()

        # Output Activation
        self.sigmoid_output = nn.Sigmoid()

        # Initialize the weights using xavier initialization
        nn.init.xavier_uniform_(self.linear_1.weight)
        nn.init.xavier_uniform_(self.linear_2.weight)
        nn.init.xavier_uniform_(self.output.weight)

    def forward(self, x, mean_mode=True):
        x = self.linear_1(x)
        x = self.lrelu_1(x)
        x = self.linear_2(x)
        x = self.lrelu_2(x)
        
        parameters = x.squeeze(-1).squeeze(-1)

        # split the activations into means and standard deviations
        halfpoint = parameters.shape[-1] // 2
        mus, sigmas = parameters[:, :halfpoint], parameters[:, halfpoint:]
        sigmas = self.sigmoid_output(sigmas)  # sigmas are restricted to be from 0 to 1

        if not mean_mode:
            # sample point from gaussian distribution
            # this is for the discriminator
            out = (torch.randn_like(mus).to(x.device) * sigmas) + mus
        else:
            out = mus
                                
        out = self.output(out)

        return out, mus, sigmas

### GAIL 
class GAIL(object):
    def __init__(self,
                 obs_dim,
                 action_dim,
                 device='cuda:0',
                 lr=3e-4,
                 ):

        self.device = device
        self.action_dim = action_dim
        latent_dim = obs_dim 
        
        self.discriminator = GAILdiscriminator(latent_dim=latent_dim,  \
                                     hidden_dim=64, action_dim=self.action_dim).to(device)


        self.lr = lr
        self.intrinsic_reward_rms = RMS(device=self.device)


        self.gail_optim = optim.Adam(
                                [
                                    {'params': self.discriminator.parameters()}
                                ],
                                lr=self.lr
                                )
        
        self.returns = None
        self.ret_rms = RunningMeanStd(shape=())
        self.ob_rms = RunningMeanStd(shape=())

    def feed_forward_generator(self,
                               b_obs, 
                               b_actions,
                               num_minibatches,
                               minibatch_size=None):

        batch_size = b_obs.shape[1]
        if minibatch_size is None:
            minibatch_size = batch_size // num_minibatches
        sampler = BatchSampler(
            SubsetRandomSampler(range(batch_size)),
            minibatch_size,
            drop_last=True)
        for indices in sampler:
            obs_batch = b_obs.view(-1, b_obs.shape[-1])[indices]
            actions_batch = b_actions.view(-1, b_actions.shape[-1])[indices]

            yield obs_batch, actions_batch

    def update(self, expert_loader, num_minibatches, b_obs, b_actions, obsfilt=None):
        policy_data_generator = self.feed_forward_generator(b_obs, b_actions, num_minibatches, expert_loader.batch_size)

        loss = 0
        n = 0
        for expert_batch, policy_batch in zip(expert_loader,
                                              policy_data_generator):
            policy_state, policy_action = policy_batch
        
            
            policy_d = self.discriminator(
                torch.cat([policy_state, policy_action], dim=1))

            expert_state, expert_action = expert_batch
            #expert_state = obsfilt(expert_state.numpy(), update=False)
            if obsfilt is not None:
                expert_state = obsfilt(expert_state.numpy(), update=False)
            #else:
            #    # Define obsfilt for atari. (copy from vecnormenv, which is used for mujoco)
            #    update_rms = False
            #    if update_rms:
            #       self.ob_rms.update(expert_state.cpu().numpy())
            #    expert_state = np.clip((expert_state.cpu().numpy() - self.ob_rms.mean) / \
            #                            np.sqrt(self.ob_rms.var + 1e-8), -5.0, 5.0)
            
            expert_state = torch.FloatTensor(expert_state).to(self.device)
            expert_action = expert_action.to(self.device)

            expert_d = self.discriminator(
                torch.cat([expert_state, expert_action], dim=1))

            expert_loss = F.binary_cross_entropy_with_logits(
                expert_d,
                torch.ones(expert_d.size()).to(self.device))
            policy_loss = F.binary_cross_entropy_with_logits( 
                policy_d,
                torch.zeros(policy_d.size()).to(self.device))

            gail_loss = expert_loss + policy_loss
            #grad_pen = self.compute_grad_pen(expert_state, expert_action,
            #                                 policy_state, policy_action)

            #loss += (gail_loss + grad_pen).item()
            loss += gail_loss.item()
            n += 1

            self.gail_optim.zero_grad()
            #(gail_loss + grad_pen).backward()
            gail_loss.backward()
            self.gail_optim.step()
        return loss / n

    def calculate_intrinsic_reward(self, state, action, 
                                   gamma,  update_rms=True, 
                       use_original_reward=True, alpha=1e-8, reward_type='log1-d'):
        with torch.no_grad():

            d = self.discriminator(torch.cat([state, action], dim=1))
            s = torch.sigmoid(d)  
            # solution from here: https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail/issues/204
            #reward = torch.log(s + alpha) - torch.log(1 - s + alpha) # only work on halfcheetah env
            if reward_type == 'logd':
                reward =  torch.log(s + alpha)
            if reward_type == 'log1-d':
                reward =  - torch.log(1 - s + alpha)
            if self.returns is None:
                self.returns = reward.clone()

            if update_rms:
                self.returns = self.returns * gamma + reward
                self.ret_rms.update(self.returns.cpu().numpy())
                self.ob_rms.update(state.cpu().numpy())

            if use_original_reward:
                return reward 
            else:
                return reward / np.sqrt(self.ret_rms.var[0] + alpha)

### VAIL
class VAIL(object):
    def __init__(self,
                 obs_dim,
                 action_dim,
                 i_c=0.2,
                 device='cuda:0',
                 lr=3e-4,
                 ):

        self.device = device
        self.i_c = i_c
        self.action_dim = action_dim 
        latent_dim = obs_dim
        
        self.discriminator = VAILdiscriminator(latent_dim=latent_dim,  \
                                     hidden_dim=64, action_dim=self.action_dim).to(device)

        self.lr = lr
        self.intrinsic_reward_rms = RMS(device=self.device)



        self.vail_optim = optim.Adam(
                                [
                                    {'params': self.discriminator.parameters()}
                                ],
                                lr=self.lr
                                )
        
        self.returns = None 
        self.ret_rms = RunningMeanStd(shape=())
        self.ob_rms = RunningMeanStd(shape=())

    def _bottleneck_loss(self, mus, sigmas, i_c=0.2, alpha=1e-8):
        """
        calculate the bottleneck loss for the given mus and sigmas
        :param mus: means of the gaussian distributions
        :param sigmas: stds of the gaussian distributions
        :param i_c: value of bottleneck
        :param alpha: small value for numerical stability
        :return: loss_value: scalar tensor
        """
        # add a small value to sigmas to avoid inf log
        kl_divergence = (0.5 * torch.sum((mus ** 2) + (sigmas ** 2)
                          - torch.log((sigmas ** 2) + alpha) - 1, dim=1))

        # calculate the bottleneck loss:
        bottleneck_loss = (torch.mean(kl_divergence) - i_c)

        # return the bottleneck_loss:
        return bottleneck_loss

    def feed_forward_generator(self,
                               b_obs, 
                               b_actions,
                               num_minibatches,
                               minibatch_size=None):

        batch_size = b_obs.shape[1]
        if minibatch_size is None:
            minibatch_size = batch_size // num_minibatches
        sampler = BatchSampler(
            SubsetRandomSampler(range(batch_size)),
            minibatch_size,
            drop_last=True)
        for indices in sampler:
            obs_batch = b_obs.view(-1, b_obs.shape[-1])[indices]
            actions_batch = b_actions.view(-1, b_actions.shape[-1])[indices]

            yield obs_batch, actions_batch

    def update(self, expert_loader, num_minibatches, b_obs, b_actions, obsfilt=None):
        policy_data_generator = self.feed_forward_generator(b_obs, b_actions, num_minibatches, expert_loader.batch_size)

        loss = 0
        n = 0
        for expert_batch, policy_batch in zip(expert_loader,
                                              policy_data_generator):
            policy_state, policy_action = policy_batch
            
            policy_d, policy_mus, policy_sigmas = self.discriminator(
                torch.cat([policy_state, policy_action], dim=1))

            expert_state, expert_action = expert_batch
            #expert_state = obsfilt(expert_state.numpy(), update=False)
            if obsfilt is not None:
                expert_state = obsfilt(expert_state.numpy(), update=False)
            #else:
            #    # Define obsfilt for atari. (copy from vecnormenv, which is used for mujoco)
            #    update_rms = False
            #    if update_rms:
            #       self.ob_rms.update(expert_state.cpu().numpy())
            #    expert_state = np.clip((expert_state.cpu().numpy() - self.ob_rms.mean) / \
            #                            np.sqrt(self.ob_rms.var + 1e-8), -5.0, 5.0)
            
            expert_state = torch.FloatTensor(expert_state).to(self.device)
            expert_action = expert_action.to(self.device)
           
            expert_d, expert_mus, expert_sigmas = self.discriminator(
                torch.cat([expert_state, expert_action], dim=1))

            expert_loss = F.binary_cross_entropy_with_logits(
                expert_d,
                torch.ones(expert_d.size()).to(self.device))
            policy_loss = F.binary_cross_entropy_with_logits( 
                policy_d,
                torch.zeros(policy_d.size()).to(self.device))
            
            
            # calculate the bottleneck_loss:
            bottle_neck_loss = self._bottleneck_loss(
                            torch.cat((expert_mus, policy_mus), dim=0),
                                        torch.cat((expert_sigmas, policy_sigmas), dim=0), self.i_c)

            vail_loss = expert_loss + policy_loss + bottle_neck_loss
            #grad_pen = self.compute_grad_pen(expert_state, expert_action,
            #                                 policy_state, policy_action)

            #loss += (gail_loss + grad_pen).item()
            loss += vail_loss.item()
            n += 1

            self.vail_optim.zero_grad()
            #(gail_loss + grad_pen).backward()
            vail_loss.backward()
            self.vail_optim.step()
        return loss / n

    def calculate_intrinsic_reward(self, state, action, gamma,  update_rms=True, 
                       use_original_reward=True, alpha=1e-8, reward_type='log1-d'):
        with torch.no_grad():
            d, _, _ = self.discriminator(torch.cat([state, action], dim=1))
            s = torch.sigmoid(d)  
            # solution from here: https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail/issues/204
            #reward = torch.log(s + alpha) - torch.log(1 - s + alpha) # only work on halfcheetah env
            if reward_type == 'logd':
                reward =  torch.log(s + alpha)
            if reward_type == 'log1-d':
                reward =  - torch.log(1 - s + alpha)
            if self.returns is None:
                self.returns = reward.clone()

            if update_rms:
                self.returns = self.returns *  gamma + reward
                self.ret_rms.update(self.returns.cpu().numpy())
                self.ob_rms.update(state.cpu().numpy())

            if use_original_reward:
                return reward 
            else:
                return reward / np.sqrt(self.ret_rms.var[0] + alpha)



### APS: Active Pretraining with Successor Features http://proceedings.mlr.press/v139/liu21b.html
#https://github.com/rll-research/url_benchmark/blob/main/agent/aps.py
class APS(object):
    def __init__(self,
                 obs_dim,
                 knn_rms=False, 
                 knn_k=12, 
                 knn_avg=True, 
                 knn_clip=0.0,
                 sf_dim=10,
                 device='cuda:0',
                 lr=3e-4,
                 ):

        self.device = device
        self.sf_dim = sf_dim 
        self.aug = nn.Identity()

        self.state_feat_net = Predictor(latent_dim=obs_dim, rnd_rep_dim=self.sf_dim, \
                                     hidden_dim=64).to(device)

        self.lr = lr
        # particle-based entropy
        rms = RMS(self.device)
        self.pbe = PBE(rms, knn_clip, knn_k, knn_avg, knn_rms,
                             self.device)


        self.aps_optim = optim.Adam(
                                [
                                    {'params': self.state_feat_net.parameters()}
                                ],
                                lr=self.lr
                                )
        
    
    def compute_aps_loss(self, next_obs_feat, task):
        """MLE loss"""
        loss = -torch.einsum("bi,bi->b", task, next_obs_feat).mean()
        return loss

    def fit_batch(self, state, action, next_state, train=True):
        task = torch.randn(self.sf_dim)
        task = task / torch.norm(task)
        task = task.cpu().unsqueeze(0)

        state_feature = state

        state_feat = self.state_feat_net(state_feature)
        state_feat = F.normalize(state_feat, dim=-1)

        loss = self.compute_aps_loss(state_feat.cpu(), task)

        if train:
            self.aps_optim.zero_grad(set_to_none=True)
            loss.backward()
            self.aps_optim.step()
        return loss

    # Calculation of the curiosity reward
    def calculate_intrinsic_reward(self, state, action, next_state, reduction='none'):
        with torch.no_grad():
            task = torch.randn(self.sf_dim)
            task = task / torch.norm(task)
            task = task.cpu().unsqueeze(0)
            
            state_feature = state

            rep = self.state_feat_net(state_feature)
            # state_feat = F.normalize(state_feat, dim=-1)

            reward = self.pbe(rep)
            intr_ent_reward = reward.reshape(-1, )

            # successor feature reward
            rep = rep / torch.norm(rep, dim=1, keepdim=True)
            intr_sf_reward = torch.einsum("bi,bi->b", task, rep.cpu()).reshape(-1, )

            reward = intr_ent_reward + intr_sf_reward.to(self.device) 

        return reward



class ICM(object):
    def __init__(self,
                 obs_dim, 
                 action_dim,
                 device='cuda:0',
                 inverse_lr=3e-4,
                 forward_lr=3e-4,
                 ):
        self.device = device

        self.inverse_model = InverseModel(latent_dim=obs_dim, action_dim=action_dim, \
                                     hidden_dim=64).to(device)
        self.forward_dynamics_model = ForwardDynamicsModel(state_dim=obs_dim, action_dim=action_dim, \
                                                      hidden_dim=64).to(device)
        self.inverse_lr = inverse_lr
        self.forward_lr = forward_lr

        self.inverse_optim = optim.Adam(lr=self.inverse_lr, params=self.inverse_model.parameters())
        self.forward_optim = optim.Adam(lr=self.forward_lr, params=self.forward_dynamics_model.parameters())

    def get_inverse_dynamics_loss(self):
        criterionID = nn.MSELoss()
        return criterionID

    def get_forward_dynamics_loss(self):
        criterionFD = nn.MSELoss()
        return criterionFD

    def fit_batch(self, state, action, next_state, train=True):

        state_feature = state
        next_state_feature = next_state
        pred_action = self.inverse_model(state_feature, next_state_feature)
        criterionID = self.get_inverse_dynamics_loss()
        inverse_loss = criterionID(pred_action, action)
        if train:
            self.inverse_optim.zero_grad()
            inverse_loss.backward(retain_graph=True)
            self.inverse_optim.step()

        # Predict the next state from the current state and the action
        pred_next_state_feature = self.forward_dynamics_model(state_feature, action)
        criterionFD = self.get_forward_dynamics_loss()
        forward_loss = criterionFD(pred_next_state_feature, next_state_feature)
        if train:
            self.forward_optim.zero_grad()
            forward_loss.backward(retain_graph=True)
            self.forward_optim.step()

        return inverse_loss, forward_loss, pred_action

    # Calculation of the curiosity reward
    def calculate_intrinsic_reward(self, state, action, next_state, reduction='none'):
        with torch.no_grad():
            state_feature = state
            next_state_feature = next_state

            pred_next_state_feature = self.forward_dynamics_model(state_feature, action)
            processed_next_state_feature = process(next_state_feature, normalize=True, range=(-1, 1))
            processed_pred_next_state_feature = process(pred_next_state_feature, normalize=True, range=(-1, 1))
            reward = F.mse_loss(processed_pred_next_state_feature, processed_next_state_feature, reduction='none')
            reward = torch.mean(reward, (0, 1, 3))

        return reward 


# ICM with unsupervised active pretraining
# https://github.com/rll-research/url_benchmark/blob/main/agent/icm_apt.py
class ICMAPT(object):
    def __init__(self,
                 obs_dim,
                 action_dim,
                 knn_rms=False, 
                 knn_k=12, 
                 knn_avg=True, 
                 knn_clip=0.0,
                 icm_rep_dim=64,
                 device='cuda:0',
                 inverse_lr=3e-4,
                 forward_lr=3e-4,
                 ):


        self.device = device
        
        self.trunk = nn.Sequential(nn.Linear(obs_dim, icm_rep_dim),
                                   nn.LayerNorm(icm_rep_dim), nn.Tanh()).to(device)
        self.inverse_model = InverseModel(latent_dim=icm_rep_dim, action_dim=action_dim, \
                                     hidden_dim=64).to(device)
        self.forward_dynamics_model = ForwardDynamicsModel(state_dim=icm_rep_dim, action_dim=action_dim, \
                                                      hidden_dim=64).to(device)
        self.inverse_lr = inverse_lr
        self.forward_lr = forward_lr

        self.inverse_optim = optim.Adam(lr=self.inverse_lr, params=self.inverse_model.parameters())
        self.forward_optim = optim.Adam(lr=self.forward_lr, params=self.forward_dynamics_model.parameters())

        rms = RMS(self.device)
        self.pbe = PBE(rms, knn_clip, knn_k, knn_avg, knn_rms,
                             self.device)


    def get_inverse_dynamics_loss(self):
        criterionID = nn.MSELoss()
        return criterionID

    def get_forward_dynamics_loss(self):
        criterionFD = nn.MSELoss()
        return criterionFD

    def fit_batch(self, state, action, next_state, train=True):

        state_feature = state
        next_state_feature = next_state
        
        state_feature = self.trunk(state_feature)
        next_state_feature = self.trunk(next_state_feature)

        pred_action = self.inverse_model(state_feature, next_state_feature)
        criterionID = self.get_inverse_dynamics_loss()
        inverse_loss = criterionID(pred_action, action)
        if train:
            self.inverse_optim.zero_grad()
            inverse_loss.backward(retain_graph=True)
            self.inverse_optim.step()

        # Predict the next state from the current state and the action
        pred_next_state_feature = self.forward_dynamics_model(state_feature, action)
        criterionFD = self.get_forward_dynamics_loss()
        forward_loss = criterionFD(pred_next_state_feature, next_state_feature)
        if train:
            self.forward_optim.zero_grad()
            forward_loss.backward(retain_graph=True)
            self.forward_optim.step()

        return inverse_loss, forward_loss, pred_action

    # Calculation of the curiosity reward
    def calculate_intrinsic_reward(self, state, action=None, next_state=None, reduction='none'):
        with torch.no_grad():
            state_feature = state

            rep = self.trunk(state_feature)
            reward = self.pbe(rep)
            reward = reward.reshape(-1, 1)

        return reward.squeeze(1)

class GIRIL(object):
    def __init__(self,
                 obs_dim,
                 action_dim,
                 device='cuda:0',
                 lr=3e-4,
                 ):

        self.device = device

        self.encoder = Encoder(latent_dim=obs_dim, action_dim=action_dim, \
                                     hidden_dim=64).to(device)
        self.forward_dynamics_model = ForwardDynamicsModel(state_dim=obs_dim, action_dim=action_dim, \
                                                      hidden_dim=64).to(device)
        self.lr = lr

        self.optim = optim.Adam(
                                [
                                    {'params': self.encoder.parameters()},
                                    {'params': self.forward_dynamics_model.parameters()}
                                ],
                                lr=self.lr
                                )
    
    def get_vae_loss(self, recon_x, x, mean, log_var):
        RECON = F.mse_loss(recon_x, x)
        KLD = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())
    
        return RECON, KLD 

    def fit_batch(self, state, action, next_state, 
                  lambda_action=1.0, kld_loss_beta=1.0, train=True):
    
        state_feature = state
        next_state_feature = next_state
        z, mu, logvar = self.encoder(state_feature, next_state_feature)
         
        reconstructed_next_state = self.forward_dynamics_model(z, state_feature)
        
        criterionAction = nn.MSELoss()
        action_loss = criterionAction(z, action)

        recon_loss, kld_loss = self.get_vae_loss(reconstructed_next_state, 
                                                 next_state_feature, mu, logvar)
        vae_loss = recon_loss + kld_loss_beta * kld_loss + lambda_action * action_loss 

        if train:
            self.optim.zero_grad()
            vae_loss.backward(retain_graph=True)
            self.optim.step()

        # return inverse_loss, forward_loss, pred_action
        return vae_loss, recon_loss, kld_loss_beta*kld_loss, \
            lambda_action*action_loss, z

    # Calculation of the curiosity reward
    def calculate_intrinsic_reward(self, state, action, next_state, reduction='none'):
        with torch.no_grad():
            state_feature = state 
            next_state_feature = next_state

            pred_next_state_feature = self.forward_dynamics_model(state_feature, action)
            processed_next_state_feature = process(next_state_feature, normalize=True, range=(-1, 1))
            processed_pred_next_state_feature = process(pred_next_state_feature, normalize=True, range=(-1, 1))
            reward = F.mse_loss(processed_pred_next_state_feature, processed_next_state_feature, reduction='none')
            # reward = torch.mean(reward, (1, 2, 3))
            reward = torch.mean(reward, (0, 1, 3))

        return reward 

class GIRILAPT(object):
    def __init__(self,
                 obs_dim,
                 action_dim,
                 knn_rms=False, 
                 knn_k=12, 
                 knn_avg=True, 
                 knn_clip=0.0,
                 icm_rep_dim=64,
                 device='cuda:0',
                 lr=3e-4,
                 ):

        self.device = device
 
        self.trunk = nn.Sequential(nn.Linear(obs_dim, icm_rep_dim),
                                   nn.LayerNorm(icm_rep_dim), nn.Tanh()).to(device)
        self.encoder = Encoder(latent_dim=icm_rep_dim, action_dim=action_dim, \
                                     hidden_dim=64).to(device)
        self.forward_dynamics_model = ForwardDynamicsModel(state_dim=icm_rep_dim, action_dim=action_dim, \
                                                      hidden_dim=64).to(device)
        self.lr = lr
        

        self.optim = optim.Adam(
                                [
                                    {'params': self.encoder.parameters()},
                                    {'params': self.forward_dynamics_model.parameters()}
                                ],
                                lr=self.lr
                                )
        rms = RMS(self.device)
        self.pbe = PBE(rms, knn_clip, knn_k, knn_avg, knn_rms,
                             self.device)
    
    def get_vae_loss(self, recon_x, x, mean, log_var):
        RECON = F.mse_loss(recon_x, x)
        KLD = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())
    
        return RECON, KLD 

    def fit_batch(self, state, action, next_state, 
                  lambda_action=1.0, kld_loss_beta=1.0, train=True):

        state_feature = state
        next_state_feature = next_state

        state_feature = self.trunk(state_feature)
        next_state_feature = self.trunk(next_state_feature)

        z, mu, logvar = self.encoder(state_feature, next_state_feature)
         
        reconstructed_next_state = self.forward_dynamics_model(z, state_feature)
        
        criterionAction = nn.MSELoss()
        action_loss = criterionAction(z, action)

        recon_loss, kld_loss = self.get_vae_loss(reconstructed_next_state, 
                                                 next_state_feature, mu, logvar)
        vae_loss = recon_loss + kld_loss_beta * kld_loss + lambda_action * action_loss 

        if train:
            self.optim.zero_grad()
            vae_loss.backward(retain_graph=True)
            self.optim.step()

        # return inverse_loss, forward_loss, pred_action
        return vae_loss, recon_loss, kld_loss_beta*kld_loss, \
            lambda_action*action_loss, z

    # Calculation of the curiosity reward
    def calculate_intrinsic_reward(self, state, action, next_state, reduction='none'):
        with torch.no_grad():
            state_feature = state

            rep = self.trunk(state_feature)
            reward = self.pbe(rep)
            reward = reward.reshape(-1, 1)

        return reward.squeeze(1)

def parse_args():
    parser = argparse.ArgumentParser()
    # PPO params
    parser = argparse.ArgumentParser()
    parser.add_argument('--env_name', type=str, default='ant')
    parser.add_argument('--seed', type=int, default=0)
    parser.add_argument('--intrinsic_module', type=str, default='icm')
    parser.add_argument('--demo_dir', type=str, default='trajs_random_elite/100episodes/')
    parser.add_argument('--num_demo', type=int, default=100)
    parser.add_argument('--reward_save_dir', type=str, default='reward_random_elite')
    parser.add_argument('--intrinsic_epoch', type=int, default=1000)
    parser.add_argument('--intrinsic_save_interval', type=int, default=50)
    parser.add_argument('--batch_size', default=32, type=int, help='batch size for loading data')
    

    # args for brax
    parser.add_argument('--env_batch_size', default=1, type=int, help='Number of parallel environments to run')

    # ppo hyperparams
    parser.add_argument('--report_interval', type=int, default=5, help='Log objective results every N updates')
    parser.add_argument('--rollout_length', type=int, default=2048,
                        help='the number of steps to run in each environment per policy rollout')
    parser.add_argument('--learning_rate', type=float, default=3e-4)
    parser.add_argument('--anneal_lr', type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
                        help='Toggle learning rate annealing for policy and value networks')
    parser.add_argument('--gamma', type=float, default=0.99, help='Discount factor for rewards')
    parser.add_argument('--gae_lambda', type=float, default=0.95, help='Lambda discount used for general advantage est')
    parser.add_argument('--num_minibatches', type=int, default=32)
    parser.add_argument('--update_epochs', type=int, default=10, help='The K epochs to update the policy')
    parser.add_argument("--norm_adv", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
                        help="Toggles advantages normalization")
    parser.add_argument("--clip_coef", type=float, default=0.2,
                        help="the surrogate clipping coefficient")
    parser.add_argument("--clip_vloss", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
                        help="Toggles whether or not to use a clipped loss for the value function, as per the paper.")
    parser.add_argument("--clip_value_coef", type=float, default=0.2,
                        help="value clipping coefficient")
    parser.add_argument("--entropy_coef", type=float, default=0.0,
                        help="coefficient of the entropy")
    parser.add_argument("--vf_coef", type=float, default=0.5,
                        help="coefficient of the value function")
    parser.add_argument("--max_grad_norm", type=float, default=0.5,
                        help="the maximum norm for the gradient clipping")
    parser.add_argument("--target_kl", type=float, default=None,
                        help="the target KL divergence threshold")
    parser.add_argument('--normalize_obs', type=lambda x: bool(strtobool(x)), default=False,
                        help='Normalize observations across a batch using running mean and stddev')
    parser.add_argument('--normalize_returns', type=lambda x: bool(strtobool(x)), default=False,
                        help='Normalize returns across a batch using running mean and stddev')
    parser.add_argument('--value_bootstrap', type=lambda x: bool(strtobool(x)), default=False,
                        help='Use bootstrap value estimates')
    parser.add_argument('--weight_decay', type=float, default=None, help='Apply L2 weight regularization to the NNs')
    parser.add_argument('--clip_obs_rew', type=lambda x: bool(strtobool(x)), default=False, help='Clip obs and rewards b/w -10 and 10')

    # QD Params
    parser.add_argument("--num_emitters", type=int, default=1, help="Number of parallel"
                                                                    " CMA-ES instances exploring the archive")
    # parser.add_argument('--grid_size', type=int, required=True, help='Number of cells per archive dimension')
    # parser.add_argument("--num_dims", type=int, required=True, help="Dimensionality of measures")
    # parser.add_argument("--popsize", type=int, required=True,
                        # help="Branching factor for each step of MEGA i.e. the number of branching solutions from the current solution point")
    parser.add_argument('--log_arch_freq', type=int, default=10,
                        help='Frequency in num iterations at which we checkpoint the archive')
    parser.add_argument('--save_scheduler', type=lambda x: bool(strtobool(x)), default=True,
                        help='Choose whether or not to save the scheduler during checkpointing. If the archive is too big,'
                             'it may be impractical to save both the scheduler and the archive_df. However, you cannot later restart from '
                             'a scheduler checkpoint and instead will have to restart from an archive_df checkpoint, which may impact the performance of the run.')
    parser.add_argument('--load_scheduler_from_cp', type=str, default=None,
                        help='Load an existing QD scheduler from a checkpoint path')
    parser.add_argument('--load_archive_from_cp', type=str, default=None,
                        help='Load an existing archive from a checkpoint path. This can be used as an alternative to loading the scheduler if save_scheduler'
                             'was disabled and only the archive df checkpoint is available. However, this can affect the performance of the run. Cannot be used together with save_scheduler')
    parser.add_argument('--total_iterations', type=int, default=100,
                        help='Number of iterations to run the entire dqd-rl loop')
    parser.add_argument('--dqd_algorithm', type=str, choices=['cma_mega_adam', 'cma_maega'],
                        help='Which DQD algorithm should be running in the outer loop')
    parser.add_argument('--expdir', type=str, help='Experiment results directory')
    parser.add_argument('--save_heatmaps', type=lambda x: bool(strtobool(x)), default=True,
                        help='Save the archive heatmaps. Only applies to archives with <= 2 measures')
    parser.add_argument('--use_surrogate_archive', type=lambda x: bool(strtobool(x)), default=False,
                        help="Use a surrogate archive at a higher resolution to get a better gradient signal for DQD")
    parser.add_argument('--sigma0', type=float, default=1.0,
                        help='Initial standard deviation parameter for the covariance matrix used in NES methods')
    parser.add_argument('--restart_rule', type=str, choices=['basic', 'no_improvement'])
    parser.add_argument('--calc_gradient_iters', type=int,
                        help='Number of iters to run PPO when estimating the objective-measure gradients (N1)')
    parser.add_argument('--move_mean_iters', type=int,
                        help='Number of iterations to run PPO when moving the mean solution point (N2)')
    parser.add_argument('--archive_lr', type=float, help='Archive learning rate for MAEGA')
    parser.add_argument('--threshold_min', type=float, default=0.0,
                        help='Min objective threshold for adding new solutions to the archive')
    parser.add_argument('--take_archive_snapshots', type=lambda x: bool(strtobool(x)), default=False,
                        help='Log the objective scores in every cell in the archive every log_freq iterations. Useful for pretty visualizations')
    parser.add_argument('--adaptive_stddev', type=lambda x: bool(strtobool(x)), default=True,
                        help='If False, the log stddev parameter in the actor will be reset on each QD iteration. Can potentially help exploration but may lose performance')


    args = parser.parse_args()
    cfg = AttrDict(vars(args))
    return cfg

if __name__ == '__main__':

    args = parse_args()

    device = torch.device("cuda:0")
    print('using device: %s' % device)
    
    args = parse_args()
    args.num_emitters = 1

    vec_env = make_vec_env_brax(args)
    args.obs_shape = vec_env.single_observation_space.shape
    args.action_shape = vec_env.single_action_space.shape
    obs_dim = args.obs_shape[0]
    action_dim = args.action_shape[0]
    print(obs_dim, action_dim)
    os.makedirs(args.reward_save_dir, exist_ok=True)
    intrinsic_log_file_name = f'{args.reward_save_dir}/reward_model_{args.intrinsic_module}_{args.env_name}.csv'
    reward_model_file_name = f'{args.reward_save_dir}/reward_model_{args.intrinsic_module}_{args.env_name}.pt'

    knn_k = 12
    if args.intrinsic_module == 'disagreement':
        reward_model = Disagreement(
                                        obs_dim,
                                        action_dim,
                                        lr=3e-4
                                        )
        
        with open(intrinsic_log_file_name, 'w') as f:
            head = 'Epoch,Batch_id,Disagreement_loss,IntrinsicReward\n'
            f.write(head)
            
        dataset, dataloader = load_sa_data(args)
        for e in range(args.intrinsic_epoch):
            for i, (state, action, next_state) in enumerate(dataloader):
                state = state.to(device)
                action = action.to(device)
                next_state = next_state.to(device)
                rnd_loss = reward_model.fit_batch(state, action, next_state)
                rewards = reward_model.calculate_intrinsic_reward(state, action, next_state)
                if (e+1) % 10 == 0:
                    print('Epoch:', e, '%s-%s Loss - Disagreement loss: %s, Rewards: %s' \
                        % ( args.intrinsic_module,  args.env_name, \
                            rnd_loss.item(), torch.mean(rewards).item()))
                
                    result_str = f'{e},{i},{rnd_loss.item()},{torch.mean(rewards).item()}\n'
                    with open(intrinsic_log_file_name, 'a') as f:
                        f.write(result_str)
            if (e+1) % args.intrinsic_save_interval == 0:
                print('Saving the pretrained %s epochs %s as %s' % (e+1, args.intrinsic_module, \
                                                                    reward_model_file_name))
                torch.save(reward_model.disagreement, reward_model_file_name)


    if args.intrinsic_module == 'rnd':
        reward_model = RND(obs_dim,
                                        rnd_rep_dim=64,
                                        lr=3e-4
                                        )
        
        with open(intrinsic_log_file_name, 'w') as f:
            head = 'Epoch,Batch_id,RND_loss,IntrinsicReward\n'
            f.write(head)
            
        dataset, dataloader = load_sa_data(args)
        for e in range(args.intrinsic_epoch):
            for i, (state, action, next_state) in enumerate(dataloader):
                state = state.to(device)
                action = action.to(device)
                next_state = next_state.to(device)
                rnd_loss = reward_model.fit_batch(state, action, next_state)
                rewards = reward_model.calculate_intrinsic_reward(state, action, next_state)
                if (e+1) % 10 == 0:
                    print('Epoch:', e, '%s-%s Loss - RND loss: %s, Rewards: %s' \
                        % ( args.intrinsic_module,  args.env_name, \
                            rnd_loss.item(), torch.mean(rewards).item()))
                
                    result_str = f'{e},{i},{rnd_loss.item()},{torch.mean(rewards).item()}\n'
                    with open(intrinsic_log_file_name, 'a') as f:
                        f.write(result_str)
            if (e+1) % args.intrinsic_save_interval == 0:
                print('Saving the pretrained %s epochs %s as %s' % (e+1, args.intrinsic_module, \
                                                                    reward_model_file_name))
                torch.save([reward_model.predictor, reward_model.target], reward_model_file_name)

    if args.intrinsic_module == 'aps':
        reward_model = APS(
                                        knn_rms=False, 
                                        knn_k=knn_k, 
                                        knn_avg=True, 
                                        knn_clip=0.0,
                                        sf_dim=10,
                                        lr=3e-4
                                        )
        
        with open(intrinsic_log_file_name, 'w') as f:
            head = 'Epoch,Batch_id,APS_loss,IntrinsicReward\n'
            f.write(head)
            
        dataset, dataloader = load_sa_data(args)
        for e in range(args.intrinsic_epoch):
            for i, (state, action, next_state) in enumerate(dataloader):
                state = state.to(device)
                action = action.to(device)
                next_state = next_state.to(device)
                aps_loss = reward_model.fit_batch(state, action, next_state)
                rewards = reward_model.calculate_intrinsic_reward(state, action, next_state)
                if (e+1) % 10 == 0:
                    print('Epoch:', e, '%s-%s Loss - APS loss: %s, Rewards: %s' \
                        % ( args.intrinsic_module,  args.env_name, \
                            aps_loss.item(), torch.mean(rewards).item()))
                
                    result_str = f'{e},{i},{aps_loss.item()},{torch.mean(rewards).item()}\n'
                    with open(intrinsic_log_file_name, 'a') as f:
                        f.write(result_str)
            if (e+1) % args.intrinsic_save_interval == 0:
                print('Saving the pretrained %s epochs %s as %s' % (e+1, args.intrinsic_module, \
                                                                    reward_model_file_name))
                torch.save(reward_model.state_feat_net, reward_model_file_name)

    if args.intrinsic_module == 'icm':
        reward_model = ICM(
                                        obs_dim,
                                        action_dim,
                                        inverse_lr=3e-4,
                                        forward_lr=3e-4)
        
        with open(intrinsic_log_file_name, 'w') as f:
            head = 'Epoch,Batch_id,Inverse_loss,Forward_loss,IntrinsicReward\n'
            f.write(head)
            
        dataset, dataloader = load_sa_data(args)
        for e in range(args.intrinsic_epoch):
            for i, (state, action, next_state) in enumerate(dataloader):
                state = state.to(device)
                action = action.to(device)
                next_state = next_state.to(device)
                inverse_loss, forward_loss, action_logit = reward_model.fit_batch(state, action, next_state)
                rewards = reward_model.calculate_intrinsic_reward(state, action, next_state)
                if (e+1) % 10 == 0:
                    print('Epoch:', e, '%s-%s Loss - Inverse loss: %s and Forward loss: %s,  Rewards: %s' \
                        % ( args.intrinsic_module,  args.env_name, \
                            inverse_loss.item(), forward_loss.item(),  torch.mean(rewards).item()))
                
                    result_str = f'{e},{i},{inverse_loss.item()},{forward_loss.item()},{torch.mean(rewards).item()}\n'
                    with open(intrinsic_log_file_name, 'a') as f:
                        f.write(result_str)
            if (e+1) % args.intrinsic_save_interval == 0:
                print('Saving the pretrained %s epochs %s as %s' % (e+1, args.intrinsic_module, \
                                                                    reward_model_file_name))
                torch.save([reward_model.inverse_model, reward_model.forward_dynamics_model], reward_model_file_name)

    if args.intrinsic_module == 'icm_apt':
        reward_model = ICMAPT(
                                        obs_dim,
                                        action_dim,
                                        knn_rms=True, 
                                        knn_k=knn_k, 
                                        knn_avg=True, 
                                        knn_clip=0.0,
                                        icm_rep_dim=64,
                                        inverse_lr=3e-4,
                                        forward_lr=3e-4)
        
        with open(intrinsic_log_file_name, 'w') as f:
            head = 'Epoch,Batch_id,Inverse_loss,Forward_loss,IntrinsicReward\n'
            f.write(head)
            
        dataset, dataloader = load_sa_data(args)
        for e in range(args.intrinsic_epoch):
            for i, (state, action, next_state) in enumerate(dataloader):
                state = state.to(device)
                action = action.to(device)
                next_state = next_state.to(device)
                inverse_loss, forward_loss, action_logit = reward_model.fit_batch(state, action, next_state)
                rewards = reward_model.calculate_intrinsic_reward(state, action, next_state)
                if (e+1) % 10 == 0:
                    print('Epoch:', e, '%s-%s Loss - Inverse loss: %s and Forward loss: %s,  Rewards: %s' \
                            % ( args.intrinsic_module,  args.env_name, \
                                inverse_loss.item(), forward_loss.item(),  torch.mean(rewards).item()))
            
                    result_str = f'{e},{i},{inverse_loss.item()},{forward_loss.item()},{torch.mean(rewards).item()}\n'
                    with open(intrinsic_log_file_name, 'a') as f:
                        f.write(result_str)

            if (e+1) % args.intrinsic_save_interval == 0:
                print('Saving the pretrained %s epochs %s as %s' % (e+1, args.intrinsic_module, \
                                                                    reward_model_file_name))
                torch.save([reward_model.inverse_model, reward_model.forward_dynamics_model], reward_model_file_name)
                    
    if args.intrinsic_module == 'giril':
        reward_model = GIRIL(
                                        obs_dim,
                                        action_dim,
                                        lr=3e-4,
                                        )
        
        with open(intrinsic_log_file_name, 'w') as f:
            head = 'Epoch,Batch_id,VAE_loss,Recon_loss,KLD_loss,Action_loss,IntrinsicReward\n'
            f.write(head)
        
        dataset, dataloader = load_sa_data(args)
        for e in range(args.intrinsic_epoch):
            for i, (state, action, next_state) in enumerate(dataloader):
                state = state.to(device)
                action = action.to(device)
                next_state = next_state.to(device)
                vae_loss, recon_loss, kld_loss, action_loss, action_logit = \
                    reward_model.fit_batch(state, action, next_state,
                                           lambda_action=100.0, kld_loss_beta=1.0)
                rewards = reward_model.calculate_intrinsic_reward(state, action, next_state)
                if (e+1) % 10 == 0:
                    print('Epoch:', e, '%s-%s Loss - VAE loss: %s, Recon loss: %s and KLD loss: %s,  Rewards: %s' \
                            % ( args.intrinsic_module,  args.env_name, \
                                vae_loss.item(), recon_loss.item(), kld_loss.item(), \
                                 torch.mean(rewards).item()))
                    result_str = f'{e},{i},{vae_loss.item()},{recon_loss.item()},{kld_loss.item()},{action_loss.item()},{torch.mean(rewards).item()}\n'
                    with open(intrinsic_log_file_name, 'a') as f:
                        f.write(result_str)
            if (e+1) % args.intrinsic_save_interval == 0:
                print('Saving the pretrained %s epochs %s as %s' % (e+1, args.intrinsic_module, \
                                                                    reward_model_file_name))
                torch.save([reward_model.encoder, reward_model.forward_dynamics_model], reward_model_file_name)
            
    if args.intrinsic_module == 'giril_apt':
        reward_model = GIRILAPT(
                                        obs_dim,
                                        action_dim,
                                        knn_rms=True, 
                                        knn_k=knn_k, 
                                        knn_avg=True, 
                                        knn_clip=0.0,
                                        lr=3e-4,
                                        )
        
        with open(intrinsic_log_file_name, 'w') as f:
            head = 'Epoch,Batch_id,VAE_loss,Recon_loss,KLD_loss,Action_loss,IntrinsicReward\n'
            f.write(head)
        
        dataset, dataloader = load_sa_data(args)
        for e in range(args.intrinsic_epoch):
            for i, (state, action, next_state) in enumerate(dataloader):
                state = state.to(device)
                action = action.to(device)
                next_state = next_state.to(device)
                vae_loss, recon_loss, kld_loss, action_loss, action_logit = \
                    reward_model.fit_batch(state, action, next_state,
                                           lambda_action=100.0, kld_loss_beta=1.0)
                rewards = reward_model.calculate_intrinsic_reward(state, action, next_state)
            
                if (e+1) % 10 == 0:
                    print('Epoch:', e, '%s-%s Loss - VAE loss: %s, Recon loss: %s and KLD loss: %s,  Rewards: %s' \
                            % ( args.intrinsic_module,  args.env_name, \
                                vae_loss.item(), recon_loss.item(), kld_loss.item(), \
                                 torch.mean(rewards).item()))
                    result_str = f'{e},{i},{vae_loss.item()},{recon_loss.item()},{kld_loss.item()},{action_loss.item()},{torch.mean(rewards).item()}\n'
                    with open(intrinsic_log_file_name, 'a') as f:
                        f.write(result_str)

            if (e+1) % args.intrinsic_save_interval == 0:
                print('Saving the pretrained %s epochs %s as %s' % (e+1, args.intrinsic_module, \
                                                                    reward_model_file_name))
                torch.save([reward_model.encoder, reward_model.forward_dynamics_model], reward_model_file_name)
            